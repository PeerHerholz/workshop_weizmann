{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://arokem.github.io/2015-ohbm-dipy-short/images/dipy-logo.png\" width=\"50%\"></center>\n",
    "\n",
    "# Structural connectivity and diffusion imaging\n",
    "\n",
    "Structural connectivity or diffusion imaging can be studied in many different way. [`Dipy`](http://nipy.org/dipy/) is only one of them. [`Dipy`](http://nipy.org/dipy/) is focusing mainly on diffusion magnetic resonance imaging (dMRI) analysis and implements a broad range of algorithms for denoising, registration, reconstruction, tracking, clustering, visualization, and statistical analysis of dMRI data. For a more detailed guide and a lot of examples go to [`Dipy`](http://nipy.org/dipy/), here we want to show you just a few basics.\n",
    "\n",
    "Speaking of which, we will be covering the following sections:\n",
    "\n",
    "1. Reconstruction of the diffusion signal with the Tensor model\n",
    "1. Other reconstruction approaches, such as sparse fascicle models (SFM)\n",
    "1. Introduction to Tractography\n",
    "1. Using Various Tissue Classifiers for Tractography\n",
    "1. Connectivity Matrices, ROI Intersections, and Density Maps\n",
    "1. Direct Bundle Registration\n",
    "\n",
    "**General disclaimer**: This notebook was written by somebody who is not an expert in diffusion imaging analysis. Therefore, certain steps might not be useful or not used in an actual analysis. Keep this in mind while going through this notebook.\n",
    "\n",
    "**<span style=\"color:red\">Important:</span>** The rendering of the diffusion images is this notebook is not possible if you run the notebook within a docker container. For this reason, all output figures were already precomputed and can be shown nonetheless. If you're running the noteobook outside of a docker container you can set the variable `view_mode` to `'create_plot'` to create plots or to `'interactive'` to interactively manipulate the 3D rendered images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_mode = 'no_render'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "Before we start with anything, let's setup the important plotting functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn import plotting\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import nibabel as nb\n",
    "import numpy as np\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore the tutorial dataset\n",
    "\n",
    "For this tutorial, we will be using the **Stanford HARDI** dataset, provided with Dipy. You can download the required datasets with the following commands:\n",
    "```python\n",
    "from dipy.data import (read_stanford_labels, read_stanford_t1,\n",
    "                       read_stanford_pve_maps)\n",
    "\n",
    "img, gtab, labels_img = read_stanford_labels()\n",
    "t1 = read_stanford_t1()\n",
    "pve_csf, pve_gm, pve_wm = read_stanford_pve_maps()\n",
    "```\n",
    "\n",
    "To speed up this process, we've already downloaded the data for you and put it into the folder: `/data/diffusion/`\n",
    "\n",
    "This means we can load all required files with the following commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = nb.load('data/diffusion/stanford_hardi/t1.nii.gz')\n",
    "pve_csf = nb.load('data/diffusion/stanford_hardi/pve_csf.nii.gz')\n",
    "pve_gm = nb.load('data/diffusion/stanford_hardi/pve_gm.nii.gz')\n",
    "pve_wm = nb.load('data/diffusion/stanford_hardi/pve_wm.nii.gz')\n",
    "img = nb.load('data/diffusion/stanford_hardi/HARDI150.nii.gz')\n",
    "labels_img = nb.load('data/diffusion/stanford_hardi/HARDI150_labels.nii.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what do we have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axial_middle = img.shape[2] // 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 4, 1).set_axis_off()\n",
    "plt.imshow(t1.get_fdata()[:, :, axial_middle].T, cmap='magma', origin='lower')\n",
    "plt.title('T1 image')\n",
    "\n",
    "plt.subplot(1, 4, 2).set_axis_off()\n",
    "plt.imshow(img.get_fdata()[:, :, axial_middle, 0].T, cmap='magma', origin='lower')\n",
    "plt.title('Without diffusion weights')\n",
    "\n",
    "plt.subplot(1, 4, 3).set_axis_off()\n",
    "plt.imshow(img.get_fdata()[:, :, axial_middle, 30].T, cmap='magma', origin='lower')\n",
    "plt.title('Diffusion weights direc. 20')\n",
    "\n",
    "plt.subplot(1, 4, 4).set_axis_off()\n",
    "plt.imshow(img.get_fdata()[:, :, axial_middle, 110].T, cmap='magma', origin='lower')\n",
    "plt.title('Diffusion weights direc. 100')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 4, 1).set_axis_off()\n",
    "plt.imshow(labels_img.get_fdata()[:, :, axial_middle].T, cmap='nipy_spectral', origin='lower')\n",
    "plt.title('Volume labels')\n",
    "\n",
    "plt.subplot(1, 4, 2).set_axis_off()\n",
    "plt.imshow(pve_csf.get_fdata()[:, :, axial_middle].T, cmap='magma', origin='lower')\n",
    "plt.title('CSF segmentation')\n",
    "\n",
    "plt.subplot(1, 4, 3).set_axis_off()\n",
    "plt.imshow(pve_gm.get_fdata()[:, :, axial_middle].T, cmap='magma', origin='lower')\n",
    "plt.title('GM segmentation')\n",
    "\n",
    "plt.subplot(1, 4, 4).set_axis_off()\n",
    "plt.imshow(pve_wm.get_fdata()[:, :, axial_middle].T, cmap='magma', origin='lower')\n",
    "plt.title('WM segmentation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In diffusion MRI (dMRI) usually we use three types of files, a Nifti file with the diffusion weighted data (here `hardi_img.nii.gz`), and two text files one with b-values and one with the b-vectors. So let's also load those two files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bvals = 'data/diffusion/stanford_hardi/HARDI150.bval'\n",
    "bvecs = 'data/diffusion/stanford_hardi/HARDI150.bvec'\n",
    "\n",
    "from dipy.core.gradients import gradient_table\n",
    "gtab = gradient_table(bvals, bvecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the GradientTable object `gtab` you can show some information about the acquisition parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gtab.info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also see the b-values using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gtab.bvals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, for example the last 10 b-vectors using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gtab.bvecs[-10:, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, that we learned how to load dMRI datasets we can start the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reconstruction of the diffusion signal with the Tensor model\n",
    "\n",
    "The diffusion tensor model is a model that describes the diffusion within a voxel. In short, for each voxel, we can estimate a tensor that had orientation and a particular tensor shape that indicates the primary diffusion directions of water in this voxel. For more details, check out the [full example](http://nipy.org/dipy/examples_built/reconst_dti.html#example-reconst-dti) on the dipy homepage.\n",
    "\n",
    "## Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = img.get_fdata()\n",
    "print('data shape before masking: (%d, %d, %d, %d)' % data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid the calculation of the Tensors in the background of the image, we first need to mask and crop the data. For this, we can use Dipy's mask module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dipy.segment.mask import median_otsu\n",
    "maskdata, mask = median_otsu(data, vol_idx=range(10,50), median_radius=3, numpass=1, autocrop=True, dilate=2)\n",
    "print('data shape after masking (%d, %d, %d, %d)' % maskdata.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor reconstruction\n",
    "\n",
    "Now that we have prepared the datasets we can go forward with the voxel reconstruction. First, we instantiate the Tensor model in the following way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dipy.reconst.dti as dti\n",
    "tenmodel = dti.TensorModel(gtab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can fit the data to the model. This is very simple. We just need to call the fit method of the `TensorModel` in the following way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tenfit = tenmodel.fit(maskdata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now that we have the model fit, we can quickly extract the fractional anisotropy (FA) or the mean diffusivity (MD) from the eigenvalues of the tensor. FA is used to characterize the degree to which the distribution of diffusion in a voxel is directional. That is, whether there is relatively unrestricted diffusion in one particular direction. The MD is simply the mean of the eigenvalues of the tensor. Since FA is a normalized measure of variance and MD is the mean, they are often used as complementary measures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dipy.reconst.dti import fractional_anisotropy, mean_diffusivity\n",
    "\n",
    "FA = fractional_anisotropy(tenfit.evals)\n",
    "MD = mean_diffusivity(tenfit.evals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what do they look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting.plot_anat(nb.Nifti1Image(FA, img.affine, img.header), cut_coords=[0,-20,0],\n",
    "                   dim=-1, draw_cross=False, cmap='magma', title='FA')\n",
    "plotting.plot_anat(nb.Nifti1Image(MD, img.affine, img.header), cut_coords=[0,-20,0],\n",
    "                   dim=-1, draw_cross=False, cmap='magma', title='MD', vmax=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also compute the colored FA or RGB-map ([Pajevic et al., 1999](https://mscl.cit.nih.gov/mscl_publications/pierpaoli_99.pdf)). First, we make sure that the FA is scaled between 0 and 1, we compute the RGB map and save it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dipy.reconst.dti import color_fa\n",
    "RGB = color_fa(FA, tenfit.evecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's visualize this\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(RGB[:, :, 31,:])\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor visualization\n",
    "\n",
    "We've estimated the tensors throughout the brain. Now, let’s try to visualize the tensor ellipsoids. For the visualization we need to load the sphere objects, define a canvas to plot the image (here a rendered window) and precut data to a small rectangular area in an axial slice of the splenium of the corpus callosum (CC)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dipy.data import get_sphere\n",
    "sphere = get_sphere('symmetric724')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evals = tenfit.evals[13:43, 44:74, 28:29]\n",
    "evecs = tenfit.evecs[13:43, 44:74, 28:29]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can color the ellipsoids using the `color_fa` values that we calculated above. In this example we additionally normalize the values to increase the contrast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfa = RGB[13:43, 44:74, 28:29]\n",
    "cfa /= cfa.max() * 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dipy.viz import window, actor\n",
    "ren = window.Scene()\n",
    "ren.add(actor.tensor_slicer(evals, evecs, scalar_colors=cfa, sphere=sphere, scale=0.3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if view_mode == 'create_plot':\n",
    "    window.record(ren, n_frames=1, size=(600, 600),\n",
    "                  out_path='data/diffusion/tensor_ellipsoids.png')\n",
    "elif view_mode == 'interactive':\n",
    "    window.show(ren)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='data/diffusion/tensor_ellipsoids.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can also visualize the tensor Orientation Distribution Functions for the same area as we did with the ellipsoids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_odfs = tenmodel.fit(data[20:50, 55:85, 38:39]).odf(sphere)\n",
    "odf_actor = actor.odf_slicer(tensor_odfs, sphere=sphere, scale=0.5, colormap=None)\n",
    "ren.add(odf_actor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if view_mode == 'create_plot':\n",
    "    window.record(ren, n_frames=1, size=(600, 600),\n",
    "                  out_path='data/diffusion/tensor_odfs.png')\n",
    "elif view_mode == 'interactive':\n",
    "    window.show(ren)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='data/diffusion/tensor_odfs.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that while the tensor model is an accurate and reliable model of the diffusion signal in the white matter, it has the drawback that it only has one principal diffusion direction. Therefore, in locations in the brain that contain multiple fiber populations crossing each other, the tensor model may indicate that the principal diffusion direction is intermediate to these directions. Therefore, using the principal diffusion direction for tracking in these locations may be misleading and may lead to errors in defining the tracks. Fortunately, other reconstruction methods can be used to represent the diffusion and fiber orientations in those locations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other reconstruction approaches\n",
    "\n",
    "Dipy offers many different reconstruction models, such as Constrained Spherical Deconvolution, Continuous Axially Symmetric Tensors, Q-Ball Constant Solid Angle, Sparse Fascicle Model and many more. For a full list check out the [Reconstruction section](http://nipy.org/dipy/examples_index.html#reconstruction) on dipy.org.\n",
    "\n",
    "But to show you how easy it is to change the reconstruction model, let's take a loop at the **Sparse Fascicle Model** ([Rokem et al., 2015](http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0123272)). In very short, sparse fascicle models (SFM) try to control for the noise due to variance, and over-fitting, by means of regularization by limiting the number of fascicles in the estimated solution.\n",
    "\n",
    "## Create Model\n",
    "\n",
    "Just like in Constrained Spherical Deconvolution (see [Reconstruction with Constrained Spherical Deconvolution](http://nipy.org/dipy/examples_built/reconst_csd.html#reconst-csd)), the SFM requires the definition of a response function. We'll take advantage of the automated algorithm in the `csdeconv` module to find this response function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dipy.reconst.csdeconv import auto_response\n",
    "response, ratio = auto_response(gtab, data, roi_radius=10, fa_thr=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `response` return value contains two entries. The first is an array with the eigenvalues of the response function and the second is the average S0 for this response. It is a very good practice to always validate the result of `auto_response`. For, this purpose we can print it and have a look at its values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initialize an SFM model object, using these values. We will use the default sphere (362 vertices, symmetrically distributed on the surface of the sphere), as a set of putative fascicle directions that are considered in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dipy.data import get_sphere\n",
    "sphere = get_sphere('symmetric362')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dipy.reconst.sfm as sfm\n",
    "sf_model = sfm.SparseFascicleModel(gtab, sphere=sphere,\n",
    "                                   l1_ratio=0.5, alpha=0.001,\n",
    "                                   response=response[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, let's focus on  the small volume of data containing parts of the corpus callosum and of the centrum semiovale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_small = data[20:50, 55:85, 38:39]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit Model & Visualize results\n",
    "\n",
    "Fitting the model to this small volume of data, we calculate the ODF of this model on the sphere and plot it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_fit = sf_model.fit(data_small)\n",
    "sf_odf = sf_fit.odf(sphere)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dipy.viz import window, actor\n",
    "ren = window.Scene()\n",
    "fodf_spheres = actor.odf_slicer(sf_odf, sphere=sphere, scale=0.8, colormap='plasma')\n",
    "ren.add(fodf_spheres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if view_mode == 'create_plot':\n",
    "    window.record(ren, size=(600, 600),\n",
    "                  out_path='data/diffusion/sf_odfs.png')\n",
    "elif view_mode == 'interactive':\n",
    "    window.show(ren)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='data/diffusion/sf_odfs.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can extract the peaks from the ODF, and plot these as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dipy.direction.peaks as dpp\n",
    "sf_peaks = dpp.peaks_from_model(sf_model,\n",
    "                                data_small,\n",
    "                                sphere,\n",
    "                                relative_peak_threshold=.5,\n",
    "                                min_separation_angle=25,\n",
    "                                return_sh=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's render these peaks as well\n",
    "fodf_peaks = actor.peak_slicer(sf_peaks.peak_dirs, sf_peaks.peak_values)\n",
    "ren.add(fodf_peaks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if view_mode == 'create_plot':\n",
    "    window.record(ren, size=(600, 600),\n",
    "                  out_path='data/diffusion/sf_peaks.png')\n",
    "elif view_mode == 'interactive':\n",
    "    window.show(ren)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='data/diffusion/sf_peaks.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can plot both the peaks and the ODFs, overlayed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fodf_spheres.GetProperty().SetOpacity(0.4)\n",
    "ren.add(fodf_spheres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if view_mode == 'create_plot':\n",
    "    window.record(ren, size=(600, 600),\n",
    "                  out_path='data/diffusion/sf_both.png')\n",
    "elif view_mode == 'interactive':\n",
    "    window.show(ren)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='data/diffusion/sf_both.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Tractography\n",
    "\n",
    "Local fiber tracking is an approach used to model white matter fibers by creating streamlines from local directional information. The idea is as follows: if the local directionality of a tract/pathway segment is known, one can integrate along those directions to build a complete representation of that structure. Local fiber tracking is widely used in the field of diffusion MRI because it is simple and robust.\n",
    "\n",
    "In order to perform local fiber tracking, three things are needed:\n",
    "1. A method for getting directions from a diffusion data set.\n",
    "1. A method for identifying different tissue types within the data set.\n",
    "1. A set of seeds from which to begin tracking.\n",
    "\n",
    "Here we will use Constrained Spherical Deconvolution (CSD) [Tournier et al., 2007](https://www.sciencedirect.com/science/article/pii/S1053811907001243) for local reconstruction and then generate deterministic streamlines using the fiber directions (peaks) from CSD and fractional anisotropic (FA) from DTI as stopping criteria for the tracking. As before, we will be using the masked Stanford HARDI dataset.\n",
    "\n",
    "## Estimate the response function and create a model\n",
    "\n",
    "For the Constrained Spherical Deconvolution we need to estimate the response function (see Reconstruction with Constrained Spherical Deconvolution) and create a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dipy.reconst.csdeconv import ConstrainedSphericalDeconvModel, auto_response\n",
    "\n",
    "response, ratio = auto_response(gtab, data, roi_radius=10, fa_thr=0.7)\n",
    "\n",
    "csd_model = ConstrainedSphericalDeconvModel(gtab, response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model fit and computation of fiber direction\n",
    "\n",
    "Next, we use `peaks_from_model` to fit the data and calculated the fiber directions in all voxels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dipy.data import get_sphere\n",
    "sphere = get_sphere('symmetric724')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dipy.segment.mask import median_otsu\n",
    "maskdata, mask = median_otsu(data, vol_idx=range(10,50), median_radius=3, numpass=1, autocrop=False, dilate=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from dipy.direction import peaks_from_model\n",
    "csd_peaks = peaks_from_model(model=csd_model,\n",
    "                             data=data,\n",
    "                             sphere=sphere,\n",
    "                             mask=mask,\n",
    "                             relative_peak_threshold=.5,\n",
    "                             min_separation_angle=25,\n",
    "                             parallel=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fiber tracking\n",
    "\n",
    "For the tracking part, we will use the fiber directions from the `csd_model` but stop tracking in areas where fractional anisotropy is low (< 0.1). To derive the FA, used here as a stopping criterion, we would need to fit a tensor model first. Here, we fit the tensor using weighted least squares (WLS)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dipy.reconst.dti import TensorModel\n",
    "tensor_model = TensorModel(gtab, fit_method='WLS')\n",
    "tensor_fit = tensor_model.fit(data, mask)\n",
    "fa = tensor_fit.fa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this simple example we can use FA to stop tracking. Here we stop tracking when FA < 0.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dipy.tracking.stopping_criterion import ThresholdStoppingCriterion\n",
    "tissue_classifier = ThresholdStoppingCriterion(fa, 0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to set starting points for propagating each track. We call those seeds. Using `random_seeds_from_mask` we can select a specific number of seeds (`seeds_count`) in each voxel where the mask `fa > 0.3` is true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dipy.tracking.utils import random_seeds_from_mask\n",
    "seeds = random_seeds_from_mask(fa > 0.5, seeds_count=1, affine=np.eye(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For quality assurance, let's visualize a slice from the direction field which we will use as the basis to perform the tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dipy.viz import window, actor\n",
    "ren = window.Scene()\n",
    "ren.add(actor.peak_slicer(csd_peaks.peak_dirs,\n",
    "                          csd_peaks.peak_values,\n",
    "                          colors=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if view_mode == 'create_plot':\n",
    "    window.record(ren, size=(600, 600),\n",
    "                  out_path='data/diffusion/csd_direction_field.png')\n",
    "elif view_mode == 'interactive':\n",
    "    window.show(ren)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='data/diffusion/csd_direction_field.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streamline generation\n",
    "\n",
    "Now that we have the direction field, we can generate the streamlines of the tractography."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dipy.tracking.local_tracking import LocalTracking\n",
    "streamline_generator = LocalTracking(csd_peaks, tissue_classifier,\n",
    "                                     seeds, affine=np.eye(4),\n",
    "                                     step_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dipy.tracking.streamline import Streamlines\n",
    "streamlines = Streamlines(streamline_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The total number of streamlines can be check as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(streamlines))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To increase the number of streamlines you can change the parameter `seeds_count` in `random_seeds_from_mask`.\n",
    "\n",
    "## Visualize streamlines\n",
    "\n",
    "Now that we have everything we can visualize the streamlines using `actor.line` or `actor.streamtube`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dipy.viz import window, actor\n",
    "ren = window.Scene()\n",
    "ren.add(actor.line(streamlines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if view_mode == 'create_plot':\n",
    "    window.record(ren, size=(600, 600),\n",
    "                  out_path='data/diffusion/det_streamlines.png')\n",
    "elif view_mode == 'interactive':\n",
    "    window.show(ren)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='data/diffusion/det_streamlines.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Various Tissue Classifiers for Tractography\n",
    "\n",
    "The tissue classifier determines if the tracking stops or continues at each tracking position. The tracking stops when it reaches an ending region (e.g. low FA, gray matter or corticospinal fluid regions) or exits the image boundaries. The tracking also stops if the direction getter has no direction to follow.\n",
    "\n",
    "In this example we want to show how you can use the white matter voxels of the corpus callosum to use as a seed mask from which the streamlines should start. For this we first need to create a corpus callosum mask. This we can get from the `labels_img`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting.plot_anat(labels_img, cut_coords=[0,-20,0], dim=-1,\n",
    "                   draw_cross=False, cmap='nipy_spectral', title='labels')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this label image file, label 2 represents the corpus callosum. So let's create a mask from this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_mask = np.array(labels_img.get_fdata()==2, dtype='int')\n",
    "plotting.plot_anat(nb.Nifti1Image(cc_mask, img.affine, img.header), cut_coords=[0,-20,0],\n",
    "                   dim=-1, draw_cross=False, cmap='magma', title='Corpus callosum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dipy.tracking import utils\n",
    "seeds = utils.seeds_from_mask(cc_mask, density=2, affine=img.affine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the mask, we can run the streamline generator again, but this time using the corpus callosum mask as a seed region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dipy.tracking.local_tracking import LocalTracking\n",
    "streamline_generator = LocalTracking(csd_peaks,\n",
    "                                     tissue_classifier,\n",
    "                                     seeds,\n",
    "                                     affine=img.affine,\n",
    "                                     step_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dipy.tracking.streamline import Streamlines\n",
    "streamlines = Streamlines(streamline_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the corpus callosum fiber tracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dipy.viz import window, actor\n",
    "ren = window.Scene()\n",
    "ren.add(actor.line(streamlines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if view_mode == 'create_plot':\n",
    "    window.record(ren, size=(600, 600),\n",
    "                  out_path='data/diffusion/deterministic.png')\n",
    "elif view_mode == 'interactive':\n",
    "    window.show(ren)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='data/diffusion/deterministic.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connectivity Matrices, ROI Intersections, and Density Maps\n",
    "\n",
    "This example is meant to be an introduction to some of the streamline tools available in Dipy. Some of the functions covered in this example are:\n",
    "\n",
    "- **`target`** allows one to filter streamlines that either pass through or do not pass through some region of the brain\n",
    "- **`connectivity_matrix`** groups and counts streamlines based on where in the brain they begin and end\n",
    "- **`density map`** counts the number of streamlines that pass through every voxel of some image.\n",
    "\n",
    "\n",
    "## Create the tractography\n",
    "\n",
    "To get started we'll need to have a set of streamlines to work with. We will use again the Stanford HARDI dataset and use `EuDX` along with the `CsaOdfModel` to make some streamlines.\n",
    "\n",
    "We'll use `peaks_from_model` to apply the `CsaOdfModel` to each white matter voxel and estimate fiber orientations which we can use for tracking:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dipy.data import read_stanford_labels, read_stanford_t1\n",
    "\n",
    "hardi_img, gtab, labels_img = read_stanford_labels()\n",
    "data = hardi_img.get_fdata()\n",
    "labels = labels_img.get_fdata()\n",
    "\n",
    "t1 = read_stanford_t1()\n",
    "t1_data = t1.get_fdata()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’ve loaded an image called labels_img which is a map of tissue types such that every integer value in the array labels represents an anatomical structure or tissue type 1. For this example, the image was created so that white matter voxels have values of either 1 or 2. We’ll use `peaks_from_model` to apply the `CsaOdfModel` to each white matter voxel and estimate fiber orientations which we can use for tracking. We will also dilate this mask by 1 voxel to ensure streamlines reach the grey matter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dipy.reconst import shm\n",
    "from dipy.direction import peaks\n",
    "from scipy.ndimage.morphology import binary_dilation\n",
    "\n",
    "white_matter = binary_dilation((labels == 1) | (labels == 2))\n",
    "csamodel = shm.CsaOdfModel(gtab, 6)\n",
    "csapeaks = peaks.peaks_from_model(model=csamodel,\n",
    "                                  data=data,\n",
    "                                  sphere=peaks.default_sphere,\n",
    "                                  relative_peak_threshold=.8,\n",
    "                                  min_separation_angle=45,\n",
    "                                  mask=white_matter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use `EuDX` to track all of the white matter. To keep things reasonably fast we use `density=1` which will result in 1 seeds per voxel. We'll set `a_low` (the parameter which determines the threshold of FA/QA under which tracking stops) to be very low because we've already applied a white matter mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dipy.tracking import utils\n",
    "from dipy.tracking.local_tracking import LocalTracking\n",
    "from dipy.tracking.stopping_criterion import BinaryStoppingCriterion\n",
    "from dipy.tracking.streamline import Streamlines\n",
    "affine = np.eye(4)\n",
    "seeds = utils.seeds_from_mask(white_matter, affine, density=1)\n",
    "stopping_criterion = BinaryStoppingCriterion(white_matter)\n",
    "\n",
    "streamline_generator = LocalTracking(csapeaks, stopping_criterion, seeds,\n",
    "                                     affine=affine, step_size=0.5)\n",
    "streamlines = Streamlines(streamline_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first of the tracking utilities we'll cover here is **`target`**. This function takes a set of streamlines and a region of interest (ROI) and returns only those streamlines that pass through the ROI. The ROI should be an array such that the voxels that belong to the ROI are `True` and all other voxels are `False` (this type of binary array is sometimes called a mask). This function can also exclude all the streamlines that pass through an ROI by setting the `include` flag to `False`.\n",
    "\n",
    "In this example, we'll target the streamlines of the corpus callosum. Our `labels` array has a sagittal slice of the corpus callosum identified by the label value 2. We'll create an ROI mask from that label and create two sets of streamlines, those that intersect with the ROI and those that don't."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "cc_slice = labels == 2\n",
    "cc_streamlines = utils.target(streamlines, affine, cc_slice)\n",
    "cc_streamlines = Streamlines(cc_streamlines)\n",
    "\n",
    "other_streamlines = utils.target(streamlines, affine, cc_slice,\n",
    "                                 include=False)\n",
    "other_streamlines = Streamlines(other_streamlines)\n",
    "assert len(other_streamlines) + len(cc_streamlines) == len(streamlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if view_mode == 'create_plot':\n",
    "    window.record(ren, size=(600, 600),\n",
    "                  out_path='data/diffusion/corpuscallosum_axial.png')\n",
    "    ren.set_camera(position=[-1, 0, 0], focal_point=[0, 0, 0], view_up=[0, 0, 1])\n",
    "    window.record(ren, size=(600, 600),\n",
    "                  out_path='data/diffusion/corpuscallosum_sagittal.png')\n",
    "elif view_mode == 'interactive':\n",
    "    window.show(ren)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='data/diffusion/corpuscallosum_axial.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='data/diffusion/corpuscallosum_sagittal.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the connectivity matrix\n",
    "\n",
    "Once we've targeted on the corpus callosum ROI, we might want to find out which regions of the brain are connected by these streamlines. To do this we can use the `connectivity_matrix` function. This function takes a set of streamlines and an array of labels as arguments. It returns the number of streamlines that start and end at each pair of labels and it can return the streamlines grouped by their endpoints. Notice that this function only considers the endpoints of each streamline. Because we're typically only interested in connections between gray matter regions, and because the label 0 represents background and the labels 1 and 2 represent white matter, we discard the first three rows and columns of the connectivity matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M, grouping = utils.connectivity_matrix(cc_streamlines, affine, labels.astype('int'),\n",
    "                                        return_mapping=True,\n",
    "                                        mapping_as_streamlines=True)\n",
    "M[:3, :] = 0\n",
    "M[:, :3] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've set `return_mapping` and `mapping_as_streamlines` to `True` so that `connectivity_matrix` returns all the streamlines in `cc_streamlines` grouped by their endpoint.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now display this matrix using matplotlib. We display it using a log scale to make small values in the matrix easier to see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(np.log1p(M), interpolation='nearest')\n",
    "plt.savefig(\"connectivity.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our example track, there are more streamlines connecting regions 11 and 54 than any other pair of regions. These labels represent the left and right superior frontal gyrus respectively. These two regions are large, close together, have lots of corpus callosum fibers and are easy to track so this result should not be a surprise to anyone.\n",
    "\n",
    "However, the interpretation of streamline counts can be tricky. The relationship between the underlying biology and the streamline counts will depend on several factors, including how the tracking was done, and the correct way to interpret these kinds of connectivity matrices is still an open question in the diffusion imaging literature.\n",
    "\n",
    "## Compute density map\n",
    "\n",
    "The next function we'll demonstrate is `density_map`. This function allows one to represent the spatial distribution of a track by counting the density of streamlines in each voxel.\n",
    "\n",
    "For example, let's take the track connecting the left and right superior frontal gyrus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_superiorfrontal_track = grouping[11, 54]\n",
    "shape = labels.shape\n",
    "dm = utils.density_map(lr_superiorfrontal_track, affine, shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save this density map and visualize it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save density map\n",
    "dm_img = nb.Nifti1Image(dm.astype(\"int16\"), img.affine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting.plot_anat(dm_img, cut_coords=[12,10,34], dim=-1, draw_cross=False,\n",
    "                   cmap='magma', title='Density Map of Region 11 & 54')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Direct Bundle Registration\n",
    "\n",
    "Creating different tractographies is cool and looks very pleasing, but what if we want to compare the tractography of two people, where the bundles don't align? No worries, Dipy has the solution: **Bundle registration**\n",
    "\n",
    "To show the concept we will use two pre-saved cingulum bundles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dipy.data import two_cingulum_bundles\n",
    "cb_subj1, cb_subj2 = two_cingulum_bundles()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm we will be using is called Streamline-based Linear Registration (SLR) [Garyfallidis et al., 2015](https://www.sciencedirect.com/science/article/pii/S1053811915003961). An important step before running the registration is to resample the streamlines so that they both have the same number of points per streamline. Here we will use 20 points. This step is not optional. Inputting streamlines with different number of points will break the theoretical advantages of using the SLR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dipy.tracking.streamline import set_number_of_points\n",
    "cb_subj1 = set_number_of_points(cb_subj1, 20)\n",
    "cb_subj2 = set_number_of_points(cb_subj2, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s say now that we want to move the `cb_subj2` (moving) so that it can be aligned with `cb_subj1` (static). Here is how this is done:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dipy.align.streamlinear import StreamlineLinearRegistration\n",
    "srr = StreamlineLinearRegistration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "srm = srr.optimize(static=cb_subj1, moving=cb_subj2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the optimization is finished we can apply the transformation to `cb_subj2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb_subj2_aligned = srm.transform(cb_subj2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So let's visualize what we did:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dipy.viz import window, actor\n",
    "ren = window.Scene()\n",
    "ren.SetBackground(1, 1, 1)\n",
    "\n",
    "bundles = [cb_subj1, cb_subj2]\n",
    "colors = [window.colors.orange, window.colors.red]\n",
    "for (i, bundle) in enumerate(bundles):\n",
    "        color = colors[i]\n",
    "        lines_actor = actor.streamtube(bundle, color, linewidth=0.3)\n",
    "        lines_actor.RotateX(-90)\n",
    "        lines_actor.RotateZ(90)\n",
    "        ren.add(lines_actor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if view_mode == 'create_plot':\n",
    "    window.record(ren, size=(600, 600),\n",
    "                  out_path='data/diffusion/before_registration.png')\n",
    "elif view_mode == 'interactive':\n",
    "    window.show(ren)\n",
    "Image(filename='data/diffusion/before_registration.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now after bundle registration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dipy.viz import window, actor\n",
    "ren = window.Scene()\n",
    "ren.SetBackground(1, 1, 1)\n",
    "\n",
    "bundles = [cb_subj1, cb_subj2_aligned]\n",
    "colors = [window.colors.orange, window.colors.red]\n",
    "for (i, bundle) in enumerate(bundles):\n",
    "        color = colors[i]\n",
    "        lines_actor = actor.streamtube(bundle, color, linewidth=0.3)\n",
    "        lines_actor.RotateX(-90)\n",
    "        lines_actor.RotateZ(90)\n",
    "        ren.add(lines_actor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if view_mode == 'create_plot':\n",
    "    window.record(ren, size=(600, 600),\n",
    "                  out_path='data/diffusion/after_registration.png')\n",
    "elif view_mode == 'interactive':\n",
    "    window.show(ren)\n",
    "Image(filename='data/diffusion/after_registration.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the two cingulum bundles are well aligned although they contain many streamlines of different length and shape."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
