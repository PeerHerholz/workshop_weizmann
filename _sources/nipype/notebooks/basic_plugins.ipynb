{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Nipype Plugins\n",
    "\n",
    "The workflow engine supports a plugin architecture for workflow execution. The available plugins allow local and distributed execution of workflows and debugging. Each available plugin is described below.\n",
    "\n",
    "Current plugins are available for Linear, Multiprocessing, [IPython](https://ipython.org/) distributed processing platforms and for direct processing on [SGE](http://www.oracle.com/us/products/tools/oracle-grid-engine-075549.html), [PBS](http://www.clusterresources.com/products/torque-resource-manager.php), [HTCondor](http://www.cs.wisc.edu/htcondor/), [LSF](http://www.platform.com/Products/platform-lsf), `OAR`, and [SLURM](http://slurm.schedmd.com/). We anticipate future plugins for the [Soma](http://brainvisa.info/soma/soma-workflow/) workflow.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "**Note**:  \n",
    "Currently, the distributed processing plugins rely on the availability of a shared filesystem across computational nodes.  \n",
    "A variety of config options can control how execution behaves in this distributed context. These are listed later on in this page.\n",
    "</div>\n",
    "\n",
    "All plugins can be executed with:\n",
    "\n",
    "```python\n",
    "workflow.run(plugin=PLUGIN_NAME, plugin_args=ARGS_DICT)\n",
    "```\n",
    "\n",
    "Optional arguments:\n",
    "\n",
    "    status_callback : a function handle\n",
    "    max_jobs : maximum number of concurrent jobs\n",
    "    max_tries : number of times to try submitting a job\n",
    "    retry_timeout : amount of time to wait between tries\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "**Note**: Except for the status_callback, the remaining arguments only apply to the distributed plugins: MultiProc / IPython(X) / SGE / PBS / HTCondor / HTCondorDAGMan / LSF\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debug\n",
    "\n",
    "This plugin provides a simple mechanism to debug certain components of a workflow without executing any node.\n",
    "\n",
    "Mandatory arguments:\n",
    "\n",
    "    callable :  A function handle that receives as arguments a node and a graph\n",
    "\n",
    "The function callable will be called for every node from a topological sort of the execution graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear\n",
    "\n",
    "This plugin runs the workflow one node at a time in a single process locally. The order of the nodes is determined by a topological sort of the workflow:\n",
    "\n",
    "```python\n",
    "workflow.run(plugin='Linear')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MultiProc\n",
    "\n",
    "Uses the [Python](http://www.python.org/) multiprocessing library to distribute jobs as new processes on a local system.\n",
    "\n",
    "Optional arguments:\n",
    "\n",
    "- `n_procs`:  Number of processes to launch in parallel, if not set number of processors/threads will be automatically detected\n",
    "\n",
    "- `memory_gb`: Total memory available to be shared by all simultaneous tasks currently running, if not set it will be automatically set to 90% of system RAM.\n",
    "\n",
    "- `raise_insufficient`: Raise exception when the estimated resources of a node exceed the total amount of resources available (memory and threads), when ``False`` (default), only a warning will be issued.\n",
    "\n",
    "- `maxtasksperchild`: number of nodes to run on each process before refreshing the worker (default: 10).\n",
    "  \n",
    "\n",
    "To distribute processing on a multicore machine, simply call:\n",
    "\n",
    "```python\n",
    "workflow.run(plugin='MultiProc')\n",
    "```\n",
    "\n",
    "This will use all available CPUs. If on the other hand, you would like to restrict the number of used resources (to say 2 CPUs), you can call:\n",
    "\n",
    "```python\n",
    "workflow.run(plugin='MultiProc', plugin_args={'n_procs' : 2}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IPython\n",
    "\n",
    "This plugin provides access to distributed computing using [IPython](https://ipython.org/) parallel machinery.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "**Note**:  \n",
    "Please read the [IPython](https://ipython.org/) documentation to determine how to set up your cluster for distributed processing. This typically involves calling ipcluster.\n",
    "</div>\n",
    "\n",
    "Once the clients have been started, any pipeline executed with:\n",
    "\n",
    "```python\n",
    "workflow.run(plugin='IPython')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGE/PBS\n",
    "\n",
    "In order to use nipype with [SGE](http://www.oracle.com/us/products/tools/oracle-grid-engine-075549.html) or [PBS](http://www.clusterresources.com/products/torque-resource-manager.php) you simply need to call:\n",
    "\n",
    "```python\n",
    "workflow.run(plugin='SGE')\n",
    "workflow.run(plugin='PBS')\n",
    "```\n",
    "\n",
    "Optional arguments:\n",
    "\n",
    "    template: custom template file to use\n",
    "    qsub_args: any other command line args to be passed to qsub.\n",
    "    max_jobname_len: (PBS only) maximum length of the job name.  Default 15.\n",
    "\n",
    "For example, the following snippet executes the workflow on myqueue with a custom template:\n",
    "\n",
    "```python\n",
    "workflow.run(plugin='SGE',\n",
    "             plugin_args=dict(template='mytemplate.sh',\n",
    "                              qsub_args='-q myqueue')\n",
    "```\n",
    "\n",
    "In addition to overall workflow configuration, you can use node level\n",
    "configuration for PBS/SGE:\n",
    "\n",
    "```python\n",
    "node.plugin_args = {'qsub_args': '-l nodes=1:ppn=3'}\n",
    "```\n",
    "\n",
    "this would apply only to the node and is useful in situations, where a particular node might use more resources than other nodes in a workflow.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "**Note**: Setting the keyword `overwrite` would overwrite any global configuration with this local configuration:  \n",
    "```node.plugin_args = {'qsub_args': '-l nodes=1:ppn=3', 'overwrite': True}```\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGEGraph\n",
    "\n",
    "SGEGraph is an execution plugin working with Sun Grid Engine that allows for  submitting the entire graph of dependent jobs at once. This way Nipype does not need to run a monitoring process - SGE takes care of this. The use of SGEGraph is preferred over SGE since the latter adds an unnecessary load on the submit machine.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "**Note**: When rerunning unfinished workflows using SGEGraph you may decide not to submit jobs for Nodes that previously finished running. This can speed up execution, but new or modified inputs that would previously trigger a Node to rerun will be ignored. The following option turns on this functionality:  \n",
    "```workflow.run(plugin='SGEGraph', plugin_args = {'dont_resubmit_completed_jobs': True})```\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSF\n",
    "\n",
    "Submitting via LSF is almost identical to SGE above except for the optional arguments field:\n",
    "\n",
    "```python\n",
    "workflow.run(plugin='LSF')\n",
    "```\n",
    "\n",
    "Optional arguments:\n",
    "\n",
    "    template: custom template file to use\n",
    "    bsub_args: any other command line args to be passed to bsub."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SLURM\n",
    "\n",
    "Submitting via SLURM is almost identical to SGE above except for the optional arguments field:\n",
    "\n",
    "```python\n",
    "workflow.run(plugin='SLURM')\n",
    "```\n",
    "\n",
    "Optional arguments:\n",
    "\n",
    "    template: custom template file to use\n",
    "    sbatch_args: any other command line args to be passed to bsub.\n",
    "    jobid_re: regular expression for custom job submission id search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SLURMGraph\n",
    "\n",
    "SLURMGraph is an execution plugin working with SLURM that allows for submitting the entire graph of dependent jobs at once. This way Nipype does not need to run a monitoring process - SLURM takes care of this. The use of SLURMGraph plugin is preferred over the vanilla SLURM plugin since the latter adds an unnecessary load on the submit machine.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "**Note**: When rerunning unfinished workflows using SLURMGraph you may decide not to submit jobs for Nodes that previously finished running. This can speed up execution, but new or modified inputs that would previously trigger a Node to rerun will be ignored. The following option turns on this functionality:  \n",
    "```workflow.run(plugin='SLURMGraph', plugin_args = {'dont_resubmit_completed_jobs': True})```\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HTCondor\n",
    "\n",
    "### DAGMan\n",
    "\n",
    "With its [DAGMan](http://research.cs.wisc.edu/htcondor/dagman/dagman.html) component, [HTCondor](http://www.cs.wisc.edu/htcondor/) (previously Condor) allows for submitting the entire graphs of dependent jobs at once (similar to SGEGraph and SLURMGraph). With the ``CondorDAGMan`` plug-in, Nipype can utilize this functionality to submit complete workflows directly and in a single step.  Consequently, and in contrast to other plug-ins, workflow execution returns almost instantaneously -- Nipype is only used to generate the workflow graph, while job scheduling and dependency resolution are entirely managed by [HTCondor](http://www.cs.wisc.edu/htcondor/).\n",
    "\n",
    "Please note that although [DAGMan](http://research.cs.wisc.edu/htcondor/dagman/dagman.html) supports specification of data dependencies as well as data provisioning on compute nodes this functionality is currently not supported by this plug-in. As with all other batch systems supported by Nipype, only HTCondor pools with a shared file system can be used to process Nipype workflows.\n",
    "\n",
    "Workflow execution with HTCondor DAGMan is done by calling:\n",
    "\n",
    "```python\n",
    "workflow.run(plugin='CondorDAGMan')\n",
    "```\n",
    "\n",
    "Job execution behavior can be tweaked with the following optional plug-in arguments. The value of most arguments can be a literal string or a filename, wherein the latter case the content of the file will be used as the argument value:\n",
    "\n",
    "- `submit_template` : submit spec template for individual jobs in a DAG (see CondorDAGManPlugin.default_submit_template for the default.\n",
    "- `initial_specs` : additional submit specs that are prepended to any job's submit file\n",
    "- `override_specs` : additional submit specs that are appended to any job's submit file\n",
    "- `wrapper_cmd` : path to an executable that will be started instead of a node script. This is useful for wrapper script that executes certain functionality prior to or after a node runs. If this option is given the wrapper command is called with the respective Python executable and the path to the node script as final arguments\n",
    "- `wrapper_args` : optional additional arguments to a wrapper command\n",
    "- `dagman_args` : arguments to be prepended to the job execution script in the dagman call\n",
    "- `block` : if True the plugin call will block until Condor has finished processing the entire workflow (default: False)\n",
    "\n",
    "Please see the [HTCondor documentation](http://research.cs.wisc.edu/htcondor/manual) for details on possible configuration options and command line arguments.\n",
    "\n",
    "Using the ``wrapper_cmd`` argument it is possible to combine Nipype workflow execution with checkpoint/migration functionality offered by, for example, [DMTCP](http://dmtcp.sourceforge.net/). This is especially useful in the case of workflows with long-running nodes, such as Freesurfer's recon-all pipeline, where Condor's job prioritization algorithm could lead to jobs being evicted from compute nodes in order to maximize overall throughput. With checkpoint/migration enabled such a job would be checkpointed prior eviction and resume work from the checkpointed state after being rescheduled -- instead of restarting from scratch.\n",
    "\n",
    "On a Debian system, executing a workflow with support for checkpoint/migration for all nodes could look like this:\n",
    "\n",
    "```python\n",
    "# define common parameters\n",
    "dmtcp_hdr = \"\"\"\n",
    "should_transfer_files = YES\n",
    "when_to_transfer_output = ON_EXIT_OR_EVICT\n",
    "kill_sig = 2\n",
    "environment = DMTCP_TMPDIR=./;JALIB_STDERR_PATH=/dev/null;DMTCP_PREFIX_ID=$(CLUSTER)_$(PROCESS)\n",
    "\"\"\"\n",
    "shim_args = \"--log %(basename)s.shimlog --stdout %(basename)s.shimout --stderr %(basename)s.shimerr\"\n",
    "# run workflow\n",
    "workflow.run(\n",
    "    plugin='CondorDAGMan',\n",
    "    plugin_args=dict(initial_specs=dmtcp_hdr,\n",
    "                     wrapper_cmd='/usr/lib/condor/shim_dmtcp',\n",
    "                     wrapper_args=shim_args)\n",
    "    )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OAR\n",
    "\n",
    "In order to use nipype with OAR you simply need to call:\n",
    "\n",
    "```python\n",
    "workflow.run(plugin='OAR')\n",
    "```\n",
    "\n",
    "Optional arguments:\n",
    "\n",
    "    template: custom template file to use\n",
    "    oar_args: any other command line args to be passed to qsub.\n",
    "    max_jobname_len: (PBS only) maximum length of the job name.  Default 15.\n",
    "\n",
    "For example, the following snippet executes the workflow on myqueue with\n",
    "a custom template:\n",
    "\n",
    "```python\n",
    "workflow.run(plugin='oar',\n",
    "             plugin_args=dict(template='mytemplate.sh',\n",
    "                              oarsub_args='-q myqueue')\n",
    "```\n",
    "\n",
    "In addition to overall workflow configuration, you can use node level configuration for OAR:\n",
    "\n",
    "```python\n",
    "node.plugin_args = {'overwrite': True, 'oarsub_args': '-l \"nodes=1/cores=3\"'}\n",
    "```\n",
    "\n",
    "this would apply only to the node and is useful in situations, where a particular node might use more resources than other nodes in a workflow. You need to set the 'overwrite' flag to bypass the general settings-template you defined for the other nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ``qsub`` emulation\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "**Note**: This plug-in is deprecated and users should migrate to the more robust and more versatile ``CondorDAGMan`` plug-in.\n",
    "</div>\n",
    "\n",
    "Despite the differences between HTCondor and SGE-like batch systems the plugin usage (incl. supported arguments) is almost identical. The HTCondor plugin relies on a ``qsub`` emulation script for HTCondor, called ``condor_qsub`` that can be obtained from a [Git repository on git.debian.org](http://anonscm.debian.org/gitweb/?p=pkg-exppsy/condor.git;a=blob_plain;f=debian/condor_qsub;hb=HEAD). This script is currently not shipped with a standard HTCondor distribution but is included in the HTCondor package from http://neuro.debian.net. It is sufficient to download this script and install it in any location on a system that is included in the ``PATH`` configuration.\n",
    "\n",
    "Running a workflow in a HTCondor pool is done by calling:\n",
    "\n",
    "```python\n",
    "workflow.run(plugin='Condor')\n",
    "```\n",
    "\n",
    "The plugin supports a limited set of qsub arguments (``qsub_args``) that cover the most common use cases. The ``condor_qsub`` emulation script translates qsub arguments into the corresponding HTCondor terminology and handles the actual job submission. For details on supported options see the manpage of ``condor_qsub``.\n",
    "\n",
    "Optional arguments:\n",
    "\n",
    "    qsub_args: any other command line args to be passed to condor_qsub."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
