{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 4: 2nd-level Analysis\n",
    "\n",
    "Last but not least, the 2nd-level analysis. After we removed left-handed subjects and normalized all subject data into template space, we can now do the group analysis. To show the flexibility of Nipype, we will run the group analysis on data with two different smoothing kernel (``fwhm= [4, 8]``) and two different normalizations (ANTs and SPM).\n",
    "\n",
    "This example will also directly include thresholding of the output, as well as some visualization.\n",
    "\n",
    "**Let's start!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group Analysis with SPM\n",
    "\n",
    "Let's first run the group analysis with the SPM normalized data.\n",
    "\n",
    "## Imports (SPM12)\n",
    "\n",
    "First, we need to import all the modules we later want to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn import plotting\n",
    "%matplotlib inline\n",
    "from os.path import join as opj\n",
    "from nipype.interfaces.io import SelectFiles, DataSink\n",
    "from nipype.interfaces.spm import (OneSampleTTestDesign, EstimateModel,\n",
    "                                   EstimateContrast, Threshold)\n",
    "from nipype.interfaces.utility import IdentityInterface\n",
    "from nipype import Workflow, Node\n",
    "from nipype.interfaces.fsl import Info\n",
    "from nipype.algorithms.misc import Gunzip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment parameters (SPM12)\n",
    "\n",
    "It's always a good idea to specify all parameters that might change between experiments at the beginning of your script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_dir = '/output'\n",
    "output_dir = 'datasink'\n",
    "working_dir = 'workingdir'\n",
    "\n",
    "# Smoothing withds used during preprocessing\n",
    "fwhm = [4, 8]\n",
    "\n",
    "# Which contrasts to use for the 2nd-level analysis\n",
    "contrast_list = ['con_0001', 'con_0002', 'con_0003', 'con_0004', 'con_0005', 'con_0006', 'con_0007']\n",
    "\n",
    "mask = \"/data/ds000114/derivatives/fmriprep/mni_icbm152_nlin_asym_09c/1mm_brainmask.nii.gz\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify Nodes (SPM12)\n",
    "\n",
    "Initiate all the different interfaces (represented as nodes) that you want to use in your workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gunzip - unzip the mask image\n",
    "gunzip = Node(Gunzip(in_file=mask), name=\"gunzip\")\n",
    "\n",
    "# OneSampleTTestDesign - creates one sample T-Test Design\n",
    "onesamplettestdes = Node(OneSampleTTestDesign(),\n",
    "                         name=\"onesampttestdes\")\n",
    "\n",
    "# EstimateModel - estimates the model\n",
    "level2estimate = Node(EstimateModel(estimation_method={'Classical': 1}),\n",
    "                      name=\"level2estimate\")\n",
    "\n",
    "# EstimateContrast - estimates group contrast\n",
    "level2conestimate = Node(EstimateContrast(group_contrast=True),\n",
    "                         name=\"level2conestimate\")\n",
    "cont1 = ['Group', 'T', ['mean'], [1]]\n",
    "level2conestimate.inputs.contrasts = [cont1]\n",
    "\n",
    "# Threshold - thresholds contrasts\n",
    "level2thresh = Node(Threshold(contrast_index=1,\n",
    "                              use_topo_fdr=True,\n",
    "                              use_fwe_correction=False,\n",
    "                              extent_threshold=0,\n",
    "                              height_threshold=0.005,\n",
    "                              height_threshold_type='p-value',\n",
    "                              extent_fdr_p_threshold=0.05),\n",
    "                    name=\"level2thresh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify input & output stream (SPM12)\n",
    "\n",
    "Specify where the input data can be found & where and how to save the output data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Infosource - a function free node to iterate over the list of subject names\n",
    "infosource = Node(IdentityInterface(fields=['contrast_id', 'fwhm_id']),\n",
    "                  name=\"infosource\")\n",
    "infosource.iterables = [('contrast_id', contrast_list),\n",
    "                        ('fwhm_id', fwhm)]\n",
    "\n",
    "# SelectFiles - to grab the data (alternativ to DataGrabber)\n",
    "templates = {'cons': opj(output_dir, 'norm_spm', 'sub-*_fwhm{fwhm_id}',\n",
    "                         'w{contrast_id}.nii')}\n",
    "selectfiles = Node(SelectFiles(templates,\n",
    "                               base_directory=experiment_dir,\n",
    "                               sort_filelist=True),\n",
    "                   name=\"selectfiles\")\n",
    "\n",
    "# Datasink - creates output folder for important outputs\n",
    "datasink = Node(DataSink(base_directory=experiment_dir,\n",
    "                         container=output_dir),\n",
    "                name=\"datasink\")\n",
    "\n",
    "# Use the following DataSink output substitutions\n",
    "substitutions = [('_contrast_id_', '')]\n",
    "subjFolders = [('%s_fwhm_id_%s' % (con, f), 'spm_%s_fwhm%s' % (con, f))\n",
    "               for f in fwhm\n",
    "               for con in contrast_list]\n",
    "substitutions.extend(subjFolders)\n",
    "datasink.inputs.substitutions = substitutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify Workflow (SPM12)\n",
    "\n",
    "Create a workflow and connect the interface nodes and the I/O stream to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiation of the 2nd-level analysis workflow\n",
    "l2analysis = Workflow(name='spm_l2analysis')\n",
    "l2analysis.base_dir = opj(experiment_dir, working_dir)\n",
    "\n",
    "# Connect up the 2nd-level analysis components\n",
    "l2analysis.connect([(infosource, selectfiles, [('contrast_id', 'contrast_id'),\n",
    "                                               ('fwhm_id', 'fwhm_id')]),\n",
    "                    (selectfiles, onesamplettestdes, [('cons', 'in_files')]),\n",
    "                    (gunzip, onesamplettestdes, [('out_file',\n",
    "                                                  'explicit_mask_file')]),\n",
    "                    (onesamplettestdes, level2estimate, [('spm_mat_file',\n",
    "                                                          'spm_mat_file')]),\n",
    "                    (level2estimate, level2conestimate, [('spm_mat_file',\n",
    "                                                          'spm_mat_file'),\n",
    "                                                         ('beta_images',\n",
    "                                                          'beta_images'),\n",
    "                                                         ('residual_image',\n",
    "                                                          'residual_image')]),\n",
    "                    (level2conestimate, level2thresh, [('spm_mat_file',\n",
    "                                                        'spm_mat_file'),\n",
    "                                                       ('spmT_images',\n",
    "                                                        'stat_image'),\n",
    "                                                       ]),\n",
    "                    (level2conestimate, datasink, [('spm_mat_file',\n",
    "                                                    '2ndLevel.@spm_mat'),\n",
    "                                                   ('spmT_images',\n",
    "                                                    '2ndLevel.@T'),\n",
    "                                                   ('con_images',\n",
    "                                                    '2ndLevel.@con')]),\n",
    "                    (level2thresh, datasink, [('thresholded_map',\n",
    "                                               '2ndLevel.@threshold')]),\n",
    "                    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the workflow (SPM12)\n",
    "\n",
    "It always helps to visualize your workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 1st-level analysis output graph\n",
    "l2analysis.write_graph(graph2use='colored', format='png', simple_form=True)\n",
    "\n",
    "# Visualize the graph\n",
    "from IPython.display import Image\n",
    "Image(filename=opj(l2analysis.base_dir, 'spm_l2analysis', 'graph.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Run the Workflow (SPM12)\n",
    "\n",
    "Now that everything is ready, we can run the 1st-level analysis workflow. Change ``n_procs`` to the number of jobs/cores you want to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2analysis.run('MultiProc', plugin_args={'n_procs': 4})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group Analysis with ANTs\n",
    "\n",
    "Now to run the same group analysis, but on the ANTs normalized images, we just need to change a few parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the SelectFiles template and recreate the node\n",
    "templates = {'cons': opj(output_dir, 'norm_ants', 'sub-*_fwhm{fwhm_id}',\n",
    "                         '{contrast_id}_trans.nii')}\n",
    "selectfiles = Node(SelectFiles(templates,\n",
    "                               base_directory=experiment_dir,\n",
    "                               sort_filelist=True),\n",
    "                   name=\"selectfiles\")\n",
    "\n",
    "# Change the substituion parameters for the datasink\n",
    "substitutions = [('_contrast_id_', '')]\n",
    "subjFolders = [('%s_fwhm_id_%s' % (con, f), 'ants_%s_fwhm%s' % (con, f))\n",
    "               for f in fwhm\n",
    "               for con in contrast_list]\n",
    "substitutions.extend(subjFolders)\n",
    "datasink.inputs.substitutions = substitutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we just have to recreate the workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiation of the 2nd-level analysis workflow\n",
    "l2analysis = Workflow(name='ants_l2analysis')\n",
    "l2analysis.base_dir = opj(experiment_dir, working_dir)\n",
    "\n",
    "# Connect up the 2nd-level analysis components\n",
    "l2analysis.connect([(infosource, selectfiles, [('contrast_id', 'contrast_id'),\n",
    "                                               ('fwhm_id', 'fwhm_id')]),\n",
    "                    (selectfiles, onesamplettestdes, [('cons', 'in_files')]),\n",
    "                    (gunzip, onesamplettestdes, [('out_file',\n",
    "                                                  'explicit_mask_file')]),\n",
    "                    (onesamplettestdes, level2estimate, [('spm_mat_file',\n",
    "                                                          'spm_mat_file')]),\n",
    "                    (level2estimate, level2conestimate, [('spm_mat_file',\n",
    "                                                          'spm_mat_file'),\n",
    "                                                         ('beta_images',\n",
    "                                                          'beta_images'),\n",
    "                                                         ('residual_image',\n",
    "                                                          'residual_image')]),\n",
    "                    (level2conestimate, level2thresh, [('spm_mat_file',\n",
    "                                                        'spm_mat_file'),\n",
    "                                                       ('spmT_images',\n",
    "                                                        'stat_image'),\n",
    "                                                       ]),\n",
    "                    (level2conestimate, datasink, [('spm_mat_file',\n",
    "                                                    '2ndLevel.@spm_mat'),\n",
    "                                                   ('spmT_images',\n",
    "                                                    '2ndLevel.@T'),\n",
    "                                                   ('con_images',\n",
    "                                                    '2ndLevel.@con')]),\n",
    "                    (level2thresh, datasink, [('thresholded_map',\n",
    "                                               '2ndLevel.@threshold')]),\n",
    "                    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can run it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2analysis.run('MultiProc', plugin_args={'n_procs': 4})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize results\n",
    "\n",
    "Now we create a lot of outputs, but how do they look like? And also, what was the influence of different smoothing kernels and normalization?\n",
    "\n",
    "**Keep in mind, that the group analysis was only done on *`N=7`* subjects, and that we chose a voxel-wise threshold of *`p<0.005`*. Nonetheless, we corrected for multiple comparisons with a cluster-wise FDR threshold of *`p<0.05`*.**\n",
    "\n",
    "So let's first look at the contrast **average**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.plotting import plot_stat_map\n",
    "%matplotlib inline\n",
    "anatimg = '/data/ds000114/derivatives/fmriprep/mni_icbm152_nlin_asym_09c/1mm_T1.nii.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_stat_map(\n",
    "    '/output/datasink/2ndLevel/ants_con_0001_fwhm4/spmT_0001_thr.nii', title='ants fwhm=4', dim=1,\n",
    "    bg_img=anatimg, threshold=2, vmax=8, display_mode='y', cut_coords=(-45, -30, -15, 0, 15), cmap='viridis');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_stat_map(\n",
    "    '/output/datasink/2ndLevel/spm_con_0001_fwhm4/spmT_0001_thr.nii', title='spm fwhm=4', dim=1,\n",
    "    bg_img=anatimg, threshold=2, vmax=8, display_mode='y', cut_coords=(-45, -30, -15, 0, 15), cmap='viridis');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_stat_map(\n",
    "    '/output/datasink/2ndLevel/ants_con_0001_fwhm8/spmT_0001_thr.nii', title='ants fwhm=8', dim=1,\n",
    "    bg_img=anatimg, threshold=2, vmax=8, display_mode='y', cut_coords=(-45, -30, -15, 0, 15), cmap='viridis');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_stat_map(\n",
    "    '/output/datasink/2ndLevel/spm_con_0001_fwhm8/spmT_0001_thr.nii', title='spm fwhm=8',\n",
    "    bg_img=anatimg, threshold=2, vmax=8, display_mode='y', cut_coords=(-45, -30, -15, 0, 15), cmap='viridis');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are more or less what you would expect: The peaks are more or less at the same places for the two normalization approaches and a wider smoothing has the effect of bigger clusters, while losing the sensitivity for smaller clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's see other contrast -- **Finger > others**. Since we removed left-handed subjects, the activation is seen on the left part of the brain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.plotting import plot_stat_map\n",
    "anatimg = '/data/ds000114/derivatives/fmriprep/mni_icbm152_nlin_asym_09c/1mm_T1.nii.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_stat_map(\n",
    "    '/output/datasink/2ndLevel/ants_con_0005_fwhm4/spmT_0001_thr.nii', title='ants fwhm=4', dim=1,\n",
    "    bg_img=anatimg, threshold=2, vmax=8, cmap='viridis', display_mode='y', cut_coords=(-45, -30, -15, 0, 15));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_stat_map(\n",
    "    '/output/datasink/2ndLevel/spm_con_0005_fwhm4/spmT_0001_thr.nii', title='spm fwhm=4', dim=1,\n",
    "    bg_img=anatimg, threshold=2, vmax=8, cmap='viridis', display_mode='y', cut_coords=(-45, -30, -15, 0, 15));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_stat_map(\n",
    "    '/output/datasink/2ndLevel/ants_con_0005_fwhm8/spmT_0001_thr.nii', title='ants fwhm=8', dim=1,\n",
    "    bg_img=anatimg, threshold=2, vmax=8, cmap='viridis', display_mode='y', cut_coords=(-45, -30, -15, 0, 15));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_stat_map(\n",
    "    '/output/datasink/2ndLevel/spm_con_0005_fwhm8/spmT_0001_thr.nii', title='spm fwhm=8', dim=1,\n",
    "    bg_img=anatimg, threshold=2, vmax=8, cmap='viridis', display_mode='y', cut_coords=(-45, -30, -15, 0, 15));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's see the results using the glass brain plotting method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.plotting import plot_glass_brain\n",
    "plot_glass_brain(\n",
    "    '/output/datasink/2ndLevel/spm_con_0005_fwhm4/spmT_0001_thr.nii', colorbar=True,\n",
    "    threshold=2, display_mode='lyrz', black_bg=True, vmax=10, title='spm_fwhm4');\n",
    "plot_glass_brain(\n",
    "    '/output/datasink/2ndLevel/ants_con_0005_fwhm4/spmT_0001_thr.nii', colorbar=True,\n",
    "    threshold=2, display_mode='lyrz', black_bg=True, vmax=10, title='ants_fwhm4');\n",
    "plot_glass_brain(\n",
    "    '/output/datasink/2ndLevel/spm_con_0005_fwhm8/spmT_0001_thr.nii', colorbar=True,\n",
    "    threshold=2, display_mode='lyrz', black_bg=True, vmax=10, title='spm_fwhm8');\n",
    "plot_glass_brain(\n",
    "    '/output/datasink/2ndLevel/ants_con_0005_fwhm8/spmT_0001_thr.nii', colorbar=True,\n",
    "    threshold=2, display_mode='lyrz', black_bg=True, vmax=10, title='ants_fwhm8');"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
